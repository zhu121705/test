Year,Work,Company,Parallel strategy,Phase,Speedup (X),,,,paper
2018,GPipe,Google,Pipeline,Training,6.3,,,,GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism
2018,PipeDream,Microsoft,Pipeline,Training,5,,,,PipeDream: Fast and Efficient Pipeline Parallel DNN Training
2019,ZeRO,Microsoft,Pipeline,Training,10,,,,ZeRO: Memory Optimizations Toward Training Trillion Parameter Models
2020,Megatron-LM,NVIDIA,Tensor,Training,2.98,,,,Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism
2021,SageMaker,Amazon,Tensor,Training,2.499094666,Interleaved Pipeline,,,Amazon SageMaker Model Parallelism: A General and Flexible Framework for Large Model Training
2022,DeepSpeed,Microsoft,Pipeline/Tensor,Inference,7.3,Interleaved Pipeline,,,DeepSpeed Inference: Enabling Efficient Inference of Transformer Models at Unprecedented Scale
2023,FSDP,Meta,Data,Training,5,,,,PyTorch FSDP: Experiences on Scaling Fully Sharded Data Parallel
2023,alpa,UCB,automated search,Inference,1.9,,,,AlpaServe: Statistical Multiplexing with Model Parallelism for Deep Learning Serving
2024,nnScaler,Microsoft,automated search,Training,3.5,,,,nnScaler: Constraint-Guided Parallelization Plan Generation for Deep Learning Training
